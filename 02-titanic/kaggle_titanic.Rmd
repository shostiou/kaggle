---
title: "Kaggle_titanic"
author: "shostiou"
date: "14/02/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

First Kaggle competition : working with the titanic dataset (which is ome kind of "sandbox")
Let's start by loading the required libraries
Another file, gender_submission shows the structure of the file to be submitted.

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(caret)
```

## Loading the data

There is one training set and one test set provided by Kaggle.

```{r, message=FALSE,warning=FALSE}
titanic_train_init <- read_csv('train.csv')
titanic_valid_init <- read_csv('test.csv')
```

In order to be able to perform cross validation, we will need to define our own test set out of the initial training set. The Caret package will be used to do the split.  

```{r}
set.seed(50977) 
i <- createDataPartition(titanic_train_init$PassengerId, p = 0.8, list=FALSE)
training <- titanic_train_init[i,]
testing <- titanic_train_init[-i,]
```

Now we will keep our testing set preciously and proceed to EDA on our training set.  

## Eploratory Data Analysis  

Checking the basic structure of the data  

```{r}
str(training)
```

Some variables need to be transformed as factors.  

```{r}
# Passenger has survided (Y/N) response variable
training$Survived <- as.factor(training$Survived)
testing$Survived <- as.factor(testing$Survived)
# Passenger class (1st to 3rd)
training$Pclass <- as.factor(training$Pclass)  
testing$Pclass <- as.factor(testing$Pclass)  
titanic_valid_init$Pclass <- as.factor(titanic_valid_init$Pclass)
# Sex
training$Sex <- as.factor(training$Sex)  
testing$Sex <- as.factor(testing$Sex)  
titanic_valid_init$Sex <- as.factor(titanic_valid_init$Sex)
# Port of embarcation  
training$Embarked <- as.factor(training$Embarked)  
testing$Embarked <- as.factor(testing$Embarked)  
titanic_valid_init$Embarked <- as.factor(titanic_valid_init$Embarked)
```

Checking the modified dataset with those factors

```{r}
str(training)
```

Let's build some plots showing relationships between predictors and response variable.  

Survived & Pclass

```{r}
ggplot(training)+
  geom_bar(mapping=aes(x=Survived,fill=Pclass))+
  facet_wrap(~Pclass)
```

It appears that passengers of 1st class had higher chances to survive whil 3rd class passengers had 1/3 chances of surviving & 2/3 of not surviving.  
The Pclass variable will be an important predictor.

Let's try to see a relationship between Sex and Surviving

```{r}
ggplot(training)+
  geom_bar(mapping=(aes(x=Survived,fill=Sex)))+
  facet_wrap(~Sex)
```

Females had much more higher chances to survive than males.  

Let see the influence of age  

```{r, warning=FALSE, message=FALSE}
ggplot(training)+
  geom_density(mapping = aes(Age,fill=Survived,col=Survived),bins=30, alpha=0.5)
```

Being a kid (under 10yo) increases chances to survive.  
While older people (over 70) has few chances to survive.  

Let see the influence of Fare 

```{r}
training %>% filter (Fare<200) %>% ggplot()+
  geom_density(mapping = aes(Fare,fill=Survived,col=Survived),bins=30, alpha=0.5)
```

Having purchased cheap tickets incresed the chances of not surviving.  

Let see the influence of the boarding port.  

```{r}
training %>% ggplot()+
  geom_bar(mapping=aes(x=Survived,fill=Embarked))+
    facet_wrap(~Embarked)
```

Embarkment port had a correlation with chances to survive. IT was better to embak at southsampton.

### parc variable

Number of parents / children aboard the Titanic

```{r}
training %>% ggplot()+
  geom_bar(mapping=aes(x=Survived,fill=Parch))+
    facet_wrap(~Parch)
```



```{r}
training %>% ggplot()+
  geom_jitter(mapping=aes(y=Survived,x=Parch))
```

### Sibsp variable

Nb of siblings / spouses aboard the Titanic
(siblings = freres et soeurs)

```{r}
training %>% ggplot()+
  geom_bar(mapping=aes(x=Survived,fill=SibSp))+
    facet_wrap(~SibSp)
```

### Checking the number of NAs


Checking the number of NAs


```{r, message=FALSE, warning=FALSE}
training %>%
  summarise_all(funs(sum(is.na(.)))) %>% t() / nrow(training)
```

There are too many data missing with the Cabin variable.  
This predictor will be ignored.  
19% of the age information is missing. A strategy has to be defined to fill the missing values. Using mean ? median ?

Ticket nb doesn't appear to have any interest for prediction.  
So is name

```{r}
median_age <- median(na.omit(training$Age))
```

Replacing NA age with median_age in the training & testing sets

```{r}
training$Age[is.na(training$Age)] <- median_age
testing$Age[is.na(testing$Age)] <- median_age
titanic_valid_init$Age[is.na(titanic_valid_init$Age)] <- median_age
```

If Embarking location is not specified, it will be replaced by Southsampton which is the predominant category.  

```{r}
training$Embarked[is.na(training$Embarked)] <- 'S'
testing$Embarked[is.na(testing$Embarked)] <- 'S'
titanic_valid_init$Embarked[is.na(titanic_valid_init$Embarked)] <- 'S'
```


## Dimensionality reduction


Classical MDS N rows (objects) x p columns (variables) each row identified by a unique row name

```{r}


d <- dist((training)) # euclidean distances between the rows
fit <- cmdscale(d,eig=TRUE, k=2) # k is the number of dim
# plot solution
x <- fit$points[,1]
y <- fit$points[,2]

ggplot()+
  geom_jitter(mapping=aes(x,y),fill=as.factor(training$Survived))

#plot(x, y, xlab="Coordinate 1", ylab="Coordinate 2",main="Metric MDS", type="n")
# text(x, y, labels = row.names(training), cex=.7)

```























## Machine Learning model definition   

### KNN - Nearest Neighbors 

```{r}
knn_train_x <- training %>% select(-Name,-Ticket,-Cabin,-PassengerId)
knn_test_x <- testing %>% select(-Name,-Ticket,-Cabin,-PassengerId)
knn_valid_x <- titanic_test_init %>% select(-Name,-Ticket,-Cabin,-PassengerId)
```

Model definition  

```{r}
ctrl <- trainControl(method="repeatedcv",repeats = 3) 
knnFit <- train(Survived ~ ., data = knn_train_x, method = "knn", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 20)
```

```{r}
knnFit
```

Validation on the test set

```{r}
knnPredict <- predict(knnFit,newdata = knn_test_x )
# Checking the confusion matrix
confusionMatrix(knnPredict, knn_test_x$Survived )

knnvalidPredict <- predict(knnFit,newdata = knn_valid_x )


```

```{r}
knn_valid_x %>%
  summarise_all(funs(sum(is.na(.)))) %>% t() / nrow(training)
```

One Fare is missing in the validation set.  
We will replace it with the median value.  

```{r}
median_fare <- median(na.omit(training$Fare))
```


Replacing NA age with median_age

```{r}
knn_valid_x$Fare[is.na(knn_valid_x$Fare)] <- median_fare
```

```{r}
knnvalidPredict <- predict(knnFit,newdata = knn_valid_x )
```



```{r}
submission <- titanic_test_init
submission$PassengerId <- titanic_test_init$PassengerId
submission$Survived <- knnvalidPredict
submission <- submission %>% select(PassengerId,Survived)
write_csv(submission,'titanic_submission_knn.csv')
```


```{r}
knnvalidtrain <- predict(knnFit)
confusionMatrix(knnvalidtrain,knn_train_x$Survived)
```






### Logistic Regression  


Model definition  

```{r}
glm.fit <- glm(Survived ~ ., data = knn_train_x, family = binomial)
glm.probs <- predict(glm.fit,type = "response",newdata = knn_test_x)
glm.pred <- ifelse(glm.probs > 0.5, "1", "0")
glm.pred <- as.factor(glm.pred)
confusionMatrix(glm.pred, testing$Survived )
```


### Random Forest  

```{r}
rf_model<-train(Survived~.,data=knn_train_x,method="rf",
                trControl=trainControl(method="cv",number=5),
                prox=TRUE,allowParallel=TRUE)
```

```{r}
rfPredict <- predict(rf_model,newdata = knn_test_x )
# Checking the confusion matrix
confusionMatrix(rfPredict, testing$Survived )
```

Good accuracy 0.8239 

## SVM  

```{r}
svm_train_x<- knn_train_x
levels(svm_train_x$Survived)=c("No","Yes")

```


```{r}
ctrl <- trainControl(method = "cv", savePred=T, classProb=T)
svm_model <- train(Survived~., data=svm_train_x, method = "svmLinear", trControl = ctrl)
```

```{r}
svm_Predict <- predict(svm_model,newdata = knn_test_x )
levels(svm_Predict)=c("0","1")
# Checking the confusion matrix
confusionMatrix(svm_Predict, testing$Survived )
```

KNN appears to be the best method.

## Models averaging


Let's see if averaging can improve the result.  

```{r}
model_average <- (as.numeric(knnPredict)-1 + as.numeric(svm_Predict)-1 + as.numeric(glm.pred)-1) / 3

```

```{r}
model_average
model_average <- ifelse(model_average > 0.5, "1", "0")
model_average <- as.factor(model_average)
```

```{r}
confusionMatrix(model_average,testing$Survived)
```

Not any better !  

In order to control model averaging, we can try to define the weights based on the accuracy score on the test set.  

```{r}
logregtestPredict <- predict(glm.fit,type = "response",newdata = knn_test_x)
logregtestPredict <- ifelse(logregtestPredict > 0.5, "1", "0")
logregtestPredict <- as.factor(logregtestPredict)
knntestPredict <- predict(knnFit,newdata = knn_test_x )
rftestPredict <- predict(rf_model,newdata = knn_test_x )
svmtestPredict <- predict(svm_model,newdata = knn_test_x )
levels(svmtestPredict)=c("0","1")

```

Confusion Matrices  

```{r}
cm_logreg_acc <- confusionMatrix(logregtestPredict,knn_test_x$Survived)$overall['Accuracy']
cm_knn_acc <- confusionMatrix(knntestPredict,knn_test_x$Survived)$overall['Accuracy']
cm_rf_acc <- confusionMatrix(rftestPredict,knn_test_x$Survived)$overall['Accuracy']
cm_svm_acc <- confusionMatrix(svmtestPredict,knn_test_x$Survived)$overall['Accuracy']

cm_sum <- cm_logreg_acc + cm_knn_acc + cm_rf_acc + cm_svm_acc

```







With the validation set 

```{r}
logregvalidPredict <- predict(glm.fit,type = "response",newdata = knn_valid_x)
logregvalidPredict <- ifelse(logregvalidPredict > 0.5, "1", "0")
logregvalidPredict <- as.factor(logregvalidPredict)
knnvalidPredict <- predict(knnFit,newdata = knn_valid_x )
rfvalidPredict <- predict(rf_model,newdata = knn_valid_x )
svmvalidPredict <- predict(svm_model,newdata = knn_valid_x )

levels(svmvalidPredict)=c("0","1")

modelpredict_average <- (as.numeric(knnvalidPredict)-1 + as.numeric(rfvalidPredict)-1 + as.numeric(svmvalidPredict)-1 + as.numeric(logregvalidPredict)-1) / 4
modelpredict_average <- ifelse(modelpredict_average > 0.5, "1", "0")
modelpredict_average <- as.factor(modelpredict_average)

```

```{r}
submission$Survived <- modelpredict_average
submission <- submission %>% select(PassengerId,Survived)
write_csv(submission,'titanic_submission_ave.csv')
```

Weighted averaging based on accuracy  

```{r}
modelpredict_w <- ((as.numeric(knnvalidPredict)-1)*cm_knn_acc + (as.numeric(rfvalidPredict)-1)*cm_rf_acc + (as.numeric(svmvalidPredict)-1)*cm_svm_acc + (as.numeric(logregvalidPredict)-1)*cm_logreg_acc) / cm_sum
modelpredict_w <- ifelse(modelpredict_w > 0.5, "1", "0")
modelpredict_w <- as.factor(modelpredict_w)

```

```{r}
submission$Survived <- modelpredict_w
submission <- submission %>% select(PassengerId,Survived)
write_csv(submission,'titanic_submission_w.csv')
```




Confusion Matrix between the 2 approaches

```{r}
confusionMatrix(modelpredict_w,modelpredict_average)
```

# Classification with SVM  

Classification is normaly the field of applications where SVM are really efficient. 

As a first step, we will define specific datasets for the SVM approach

```{r}
svm_train <- knn_train_x
svm_test <- knn_test_x
svm_valid <- titanic_test_init %>% select(-Name,-Ticket,-Cabin,-PassengerId)
```


Let's start by training a linear SVM


```{r}
library(kernlab)
```

```{r}
svm_lin_class <- ksvm(Survived~ .,data=svm_train,
                      kernel = "vanilladot")
```

```{r}
svm_lin_class
```

In the next step, we will evaluate the performance of the model  

```{r}
svm_lin_pred <- predict(svm_lin_class,newdata=svm_test,type="response")
```

Confusion Matrix

```{r}
confusionMatrix(svm_lin_pred,svm_test$Survived)
```

We will continue with a RBF Kernel

```{r}
svm_rbf_class <- ksvm(Survived~ .,data=svm_train,
                      kernel = "rbfdot")
svm_rbf_class
```

```{r}
svm_rbf_pred <- predict(svm_rbf_class,newdata=svm_test,type="response")
```

Confusion Matrix

```{r}
confusionMatrix(svm_rbf_pred,svm_test$Survived)
```

Trying to optimize the cost values C  

```{r}
cost_values <- c(1,seq(from = 2, to = 3, by = 0.1))

accuracy_values <- sapply(cost_values, function(x){
  set.seed(12345)
  m <- ksvm(Survived ~ ., data=svm_train, kernel="rbfdot", C=x)
  pred <- predict(m, svm_test)
  agree <- ifelse(pred==svm_test$Survived,1,0)
  accuracy <- sum(agree) / nrow(svm_test)
  return(accuracy)
}  )

```

```{r}
plot(cost_values,accuracy_values,type="b")
```

The best C = 2.5

```{r}
svm_rbf_class <- ksvm(Survived~ .,data=svm_train,
                      kernel = "rbfdot", C=2.5)
svm_rbf_class
```

```{r}
svm_rbf_pred <- predict(svm_rbf_class,newdata=svm_test,type="response")
confusionMatrix(svm_rbf_pred,svm_test$Survived)

```



# Neural Network  

```{r}
install.packages('taRifx')
```

```{r}
library(taRifx)
```


```{r}
nn_train <- remove.factors(knn_train_x)
nn_test <- remove.factors(knn_test_x)
nn_valid <- titanic_test_init %>% select(-Name,-Ticket,-Cabin,-PassengerId)
```

Calling one NN library  

```{r}
library(neuralnet)
```

Normalization of the IOs

```{r}
normalize <- function(x){
  return((x-min(x))/(max(x)-min(x)))
} 
```



```{r}
nn_class <- neuralnet(Survived~ Age+Fare,data=(nn_train))
plot(nn_class)
```



