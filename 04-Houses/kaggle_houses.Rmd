---
title: "Houses"
author: "shostiou"
date: "20/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Import des fichiers csv

Import des bibliotheques

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(caret)
```

```{r, message=FALSE,warning=FALSE}
train <- read_csv('data/train.csv')
test <- read_csv('data/test.csv')
```

Creation d'un ensemble de training et de validation

```{r}
set.seed(50977) 
i <- createDataPartition(train$Id, p = 0.8, list=FALSE)
training <- train[i,]
validation <- train[-i,]
```

## Exploration des donnees


```{r}
str(training)
```

Les noms de colonnes integrant des num en en tete doivent etre modifies afin d'eviter des incoherences de traitement avec Tidyverse

```{r}
colnames(training)[colnames(training) == '1stFlrSF'] <- 'FstFlrSF'
colnames(training)[colnames(training) == '2ndFlrSF'] <- 'SndFlrSF'
colnames(training)[colnames(training) == '3SsnPorch'] <- 'TrdSsnPorch'

colnames(validation)[colnames(validation) == '1stFlrSF'] <- 'FstFlrSF'
colnames(validation)[colnames(validation) == '2ndFlrSF'] <- 'SndFlrSF'
colnames(validation)[colnames(validation) == '3SsnPorch'] <- 'TrdSsnPorch'

colnames(test)[colnames(test) == '1stFlrSF'] <- 'FstFlrSF'
colnames(test)[colnames(test) == '2ndFlrSF'] <- 'SndFlrSF'
colnames(test)[colnames(test) == '3SsnPorch'] <- 'TrdSsnPorch'



```




En analysant le champ des donnees, on peut deja identifier un certain nombre de fuites (data leakage) dont l'elimination sera necessaire :  
- SaleType : Type of sale  
- SaleCondition: Condition of sale  
- YrSold: Year Sold (YYYY)  
- MoSold: Month Sold (MM)  

```{r}
# Suppression des champs lies au data leakage  
training <- training %>% select (-SaleType,-SaleCondition, -YrSold, -MoSold)
validation <- validation %>% select (-SaleType,-SaleCondition, -YrSold, -MoSold)
test <- test %>% select (-SaleType,-SaleCondition, -YrSold, -MoSold)
```

Toutes les variables encodees sous forme de caractere sont en fait de variables categorielles. Il donc necessaire de les convertir en factors.  

```{r}
# Conversion sous forme de factor
training[sapply(training, is.character)] <- lapply(training[sapply(training, is.character)], as.factor)

validation[sapply(validation, is.character)] <- lapply(validation[sapply(validation, is.character)], as.factor)

test[sapply(test, is.character)] <- lapply(test[sapply(test, is.character)], as.factor)

```


Certaines valeurs numeriques sont en definitive, des donnees categorielles.   
Elles doivent donc être traitees en tant que Factor.  

MSSubClass: Identifies the type of dwelling involved in the sale.  
OverallQual: Rates the overall material and finish of the house
OverallCond: Rates the overall condition of the house

```{r}
training$MSSubClass <- as.factor(training$MSSubClass)
validation$MSSubClass <- as.factor(validation$MSSubClass)
test$MSSubClass <- as.factor(test$MSSubClass)

training$OverallQual <- as.factor(training$OverallQual)
validation$OverallQual <- as.factor(validation$OverallQual)
test$OverallQual <- as.factor(test$OverallQual)


training$OverallCond <- as.factor(training$OverallCond)
validation$OverallCond <- as.factor(validation$OverallCond)
test$OverallCond <- as.factor(test$OverallCond)

```

## On supprime les variables factor a deux niveau

```{r}
# Recherche des variables de type factor
df_factor <- training[,sapply(training, is.factor)]
col_2factor <- colnames(df_factor[,sapply(df_factor, nlevels)<2])

```




## Recherche des elements manquants

- Compte tenu de la part important de NAs sur les champs PoolQC, MiscFeature, Alley on peut considérer ces grandeurs comme non pertinentes.  
- Une stratégie de remplacement des grandeurs LotFrontage, Garagetype, GareaYrBlt, GarageFinis, GarageQaul, GaraCond, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, MasVnrType, MasVnrArea devra être mise en place.  
- La question reste ouverte au sujet de : Fence, FireplaceQu, LotFrontage.  



```{r}
sort(round(sapply(training,function(x) sum(is.na (x)))/nrow(training)*100,0), decreasing = TRUE)
```

Suppression des champs inutiles

Vérification de l'incidence des champs sur le prix de vente

Le graphe comfirme l'absence de necessite de conserver le champ.

```{r}
# PoolQC
training %>% select(PoolQC,SalePrice) %>% ggplot() +
  geom_jitter(mapping = aes(x=PoolQC,y=SalePrice))
```

Idem pour MiscFeature

```{r}
# MiscFeature
training %>% select(MiscFeature,SalePrice) %>% ggplot() +
  geom_jitter(mapping = aes(x=MiscFeature,y=SalePrice))
```

Même constat pour Alley

```{r}
# Alley
training %>% select(Alley,SalePrice) %>% ggplot() +
  geom_jitter(mapping = aes(x=Alley,y=SalePrice))
```


```{r}
training <- training %>% select (-PoolQC,-MiscFeature, -Alley)
validation <- validation %>% select (-PoolQC,-MiscFeature, -Alley)
test <- test %>% select (-PoolQC,-MiscFeature, -Alley)
```



Focalisation sur les donnees Fence, FireplaceQu

```{r}
# Fence
training %>% select(Fence,SalePrice) %>% ggplot() +
  geom_boxplot(mapping = aes(x=Fence,y=SalePrice))
```

```{r}
# FireplaceQu
training %>% select(FireplaceQu,SalePrice) %>% ggplot() +
  geom_boxplot(mapping = aes(x=FireplaceQu,y=SalePrice))
```

Cette variable mérite d'être conservée.

FireplaceQu est lie a Fireplaces. Pas de donnees manquantes si presence d'une cheminee.

```{r}
training %>% select(FireplaceQu,Fireplaces) %>% filter (Fireplaces > 0) %>% filter(is.na(FireplaceQu))
```

```{r}
# Recherche des variables numériques uniquement
cor_df_training <-(training[,unlist(lapply(training, is.numeric))])
```





Prochaine étape : normaliser toutes les variables numeriques.
La fonction preProcess de caret peut etre utilisee

```{r}
X_train <- training %>% select(-SalePrice)
y_train <- training %>% select(SalePrice)
X_valid <- validation %>% select(-SalePrice)
y_valid <- validation %>% select(SalePrice)

preProcValues <- preProcess(X_train, method = c("center", "scale"))
X_train_tf <- predict(preProcValues, X_train)
X_valid_tf <- predict(preProcValues, X_valid)
X_test_tf <- predict(preProcValues, test)
```

Recherche des correlations

```{r, message=TRUE}

df <- data.frame()
for (i in colnames(X_train[,unlist(lapply(X_train, is.numeric))]))
  {
  df <- rbind(df,cor(X_train[,i],y_train))
}
df <- df %>% arrange(desc(SalePrice))

```

Lets's focus on the most highly correlated features to the output variable

```{r}
df
```

```{r}
training %>% ggplot() +
  geom_jitter(mapping = aes(x=GrLivArea,y=log(SalePrice)))
```

```{r}
training %>% ggplot() +
  geom_jitter(mapping = aes(x=GarageCars,y=log(SalePrice)))
```


```{r}
training %>% ggplot() +
  geom_jitter(mapping = aes(x=GarageArea,y=log(SalePrice)))
```

```{r}

training %>% ggplot() +
  geom_jitter(mapping = aes(x=EnclosedPorch,y=log(SalePrice)))
```

## Recherche de Simplification 

PCA valeur par defaut 95% de la variance

```{r}
preProcess(training, method = "pca")
```
## Creation d'un modele lineaire



```{r}
df <- X_train_tf
df$SalePrice <- y_train
colnames(df)[colnames(df) == 'SalePrice.SalePrice'] <- 'SalePrice'
# Remove index
df <- df %>% select(-Id)
house_lm <- lm(training$SalePrice ~ MSSubClass + MSZoning + LotFrontage+ 
                 LotArea+Street+LotShape, data = df)
summary(house_lm)
```






